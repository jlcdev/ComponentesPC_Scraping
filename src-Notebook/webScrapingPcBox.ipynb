{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import re\n",
    "import csv\n",
    "import configparser\n",
    "import os\n",
    "from os.path  import basename\n",
    "from logging.config import fileConfig\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read config file and init vars\n",
    "def initialize():\n",
    "    global companyName\n",
    "    global target_urls\n",
    "    global baseUrl\n",
    "    global csvName\n",
    "    global timeOut\n",
    "    global minInterval\n",
    "    global categories\n",
    "    global score\n",
    "    global responseTimeThreshold\n",
    "    global headers\n",
    "    global productPage\n",
    "    global calcIntervalDelay \n",
    "    \n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('properties.config')\n",
    "\n",
    "    companyName=config['pcBox']['companyName']\n",
    "    target_urls=eval(config['pcBox']['companyUrl'])\n",
    "    baseUrl=config['pcBox']['baseUrl']\n",
    "    csvName=config['pcBox']['csvName']\n",
    "    timeOut=int(config['pcBox']['timeOut'])\n",
    "    minInterval=float(config['pcBox']['minInterval'])\n",
    "    csvName=config['pcBox']['csvName']\n",
    "    categories=eval(config['pcBox']['category'])\n",
    "    score=config['pcBox']['score']\n",
    "    responseTimeThreshold=float(config['pcBox']['responseTimeThreshold'])\n",
    "    # Simulate a Chrome 86 version WEB BROWSER\n",
    "    headers = {\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, sdch, br\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.8\",\n",
    "    \"Cache-Control\": \"no-cache\",\n",
    "    \"dnt\": \"1\",\n",
    "    \"Pragma\": \"no-cache\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    # Will keep all products within a page\n",
    "    productPage = []\n",
    "\n",
    "    # No timeDelay between requests unless Site starts performing badly.\n",
    "    calcIntervalDelay=0\n",
    "\n",
    "    # Prepare dataSet in CSV file.\n",
    "    with open(csvName, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['timestamp','company_name','name', 'brand_name', 'category','product_number', 'price', 'score', 'image_url','image_path']\n",
    "        productwriter = csv.DictWriter(csvfile, delimiter=',', fieldnames=fieldnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send_page_request function send HTTP request and control Response times.\n",
    "# Input paramters: URL, header to be sent, timeOut and interval to wait before launching request.\n",
    "# Return soup object with the page.\n",
    "def send_page_request(url,headers,timeOut,interval):\n",
    "    if (interval>0):\n",
    "        time.sleep(interval) \n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        page=requests.get(url,headers=headers,timeout=timeOut)\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"Page TimeOut. Sleep for 5min\")\n",
    "        time.sleep(300)\n",
    "        page=requests.get(url,headers=headers,timeout=timeOut)\n",
    "        pass\n",
    "    except requests.exceptions.RequestException:\n",
    "        pass\n",
    "\n",
    "    # Response_time in seconds     \n",
    "    t1 = time.time() - t0\n",
    "    # t1 is time elapsed in request. Needs to be compared with predefined threshold.\n",
    "    if (t1>responseTimeThreshold):\n",
    "        # Update Calculated Interval Delay as 3 times response times\n",
    "        calcIntervalDelay=t1*3\n",
    "    else:\n",
    "        # Back to 0 if response times gets better - below predefined threshold\n",
    "        calcIntervalDelay=0\n",
    "    \n",
    "    if page:\n",
    "        try:\n",
    "            soupContent = BeautifulSoup(page.content,\"html5lib\")\n",
    "        except:\n",
    "            print('Error. No valid page available.')\n",
    "    \n",
    "    return soupContent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get product detail for a particular product or item.\n",
    "def get_products_pageDetail(soupContent):\n",
    "    productDetail={}\n",
    "    productRef=soupContent.find('p', attrs={'class':'referenciasf'})\n",
    "    # Remove tabs or cr from strings\n",
    "    productRefText = re.sub(r\"[\\n\\t\\r]*\", \"\", productRef.get_text())\n",
    "    productNumber=(productRefText.split(\":\")[1]).split()[0]\n",
    "    productBrandName=(productRefText.split(\":\")[-1])\n",
    "    productDetail['productNumber']=productNumber\n",
    "    productDetail['productBrandName']=productBrandName\n",
    "    return productDetail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get main product in soup object.\n",
    "def get_products_page(soupContent):\n",
    "    productList=[]\n",
    "    for product in soupContent.findAll('div',attrs={'class':'col-xs-6 col-sm-4 col-md-3'}):\n",
    "        # Get Image Area - Extract ProductDetailUrl and ImageUrl\n",
    "        productImageArea=product.find('figure', attrs={'class':'product-image-area'})\n",
    "        productDetailUrl=productImageArea.find('a', attrs={'class':'product-image'}).get('href')\n",
    "        imageUrl=productImageArea.find('img').get('data-src')\n",
    "\n",
    "        # Show ProductUrl,ProductName, Price, ImageUrl, Disponibilidad\n",
    "        productName=product.find('h2', attrs={'class':'product-name'}).a.get('title')\n",
    "        productName = re.sub(r\"[\\r\\n\\t]*\", \"\", productName)\n",
    "\n",
    "        productPrice=product.find('span', attrs={'class':'product-price'}).get_text()\n",
    "        productPrice = re.sub(r\"[\\r\\n\\tâ‚¬]*\", \"\", productPrice)\n",
    "\n",
    "        # Product Available string. Not part of current DataSet.\n",
    "        productDisp=product.find('div', attrs={'class':'product-disponibilidad signica'}).get_text()\n",
    "        productDisp = re.sub(r\"[\\r\\n\\t]*\", \"\", productDisp)\n",
    "    \n",
    "        # Get Product Page Detail - productNumber y productBrandName\n",
    "        soupDetail=send_page_request(baseUrl+productDetailUrl,headers,timeOut,minInterval+calcIntervalDelay)\n",
    "        productDetail=get_products_pageDetail(soupDetail)\n",
    "        \n",
    "        #print(productDetailUrl)\n",
    "        #print(productName)\n",
    "        #print(productPrice)\n",
    "        #print(imageUrl)\n",
    "        #print(productDisp)\n",
    "        #print(\"P/N:\",productDetail['productNumber'])\n",
    "        #print(\"BrandName:\",productDetail['productBrandName'])\n",
    "        #print(\"------------------\")\n",
    "    \n",
    "        productList.append({\n",
    "            'timestamp':time.time(),\n",
    "            'company_name':companyName,\n",
    "            'name':productName,\n",
    "            'brand_name':productDetail['productBrandName'],\n",
    "            'category':category,\n",
    "            'product_number':productDetail['productNumber'],\n",
    "            'price':productPrice,\n",
    "            'score':score,\n",
    "            'image_url':imageUrl,\n",
    "            'image_path':imageUrl\n",
    "        })\n",
    "    return productList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write products dictionary to csv file\n",
    "def to_csv(products, name_csv):\n",
    "    with open(name_csv, 'a', newline='') as csvfile:\n",
    "        fieldnames = ['timestamp','company_name','name', 'brand_name', 'category','product_number', 'price', 'score', 'image_url','image_path']\n",
    "        productwriter = csv.DictWriter(csvfile, delimiter=',', fieldnames=fieldnames)\n",
    "        for product in products:\n",
    "            productwriter.writerow(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download images based on URL.\n",
    "def download_images(products):\n",
    "    if len(products) < 1:\n",
    "        return products\n",
    "    url_list = list(map(lambda x: x['image_url'], products))\n",
    "    path = './images/' + products[0]['category'] + '/'\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except:\n",
    "        print('path exist')\n",
    "    for i, url in enumerate(url_list):\n",
    "        with open(path+basename(url), \"wb\") as f:\n",
    "            f.write(requests.get(url).content)\n",
    "        products[i]['image_path'] = path+basename(url)\n",
    "        time.sleep(0.5)\n",
    "    return products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check pagination in Page.\n",
    "# If pagination tag is found return list of subpages. If not, return False.\n",
    "def get_pagination(soupContent):\n",
    "    pagination=soupContent.find('ul',attrs={'class':'pagination'})\n",
    "    subpages=[]\n",
    "    initial=1\n",
    "    if pagination:\n",
    "        for subpage in pagination.findAll('a',attrs={'class':''}):\n",
    "            #if it is a subpage add to list.\n",
    "            if subpage.get_text():\n",
    "                subpages.append(subpage['href'])\n",
    "                print(subpage['href'])\n",
    "                print(int(subpage.get_text()))\n",
    "                i=initial\n",
    "                initial +=1\n",
    "                while i<int(subpage.get_text()):\n",
    "                    subpages.append(subpage['href'].replace(subpage.get_text(),str(i)))\n",
    "                    i += 1\n",
    "        return subpages\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Timestamp :  2020-11-07 17:09:45\n",
      "Requesting Target URL:  https://www.pcbox.com/categorias/procesadores?nodo=18\n",
      "Category:  Procesadores\n",
      "path exist\n",
      "Requesting Target URL:  https://www.pcbox.com/categorias/memoria-ram?nodo=23\n",
      "Category:  Memoria\n",
      "path exist\n",
      "Requesting Target URL:  https://www.pcbox.com/categorias/placas-base?nodo=17\n",
      "Category:  Placas Base\n",
      "path exist\n",
      "Requesting Target URL:  https://www.pcbox.com/categorias/discos-duros?nodo=19\n",
      "Category:  Discos Duros\n",
      "path exist\n",
      "Requesting Target URL:  https://www.pcbox.com/categorias/discos-ssd?nodo=106\n",
      "Category:  Discos Duros\n",
      "path exist\n",
      "Requesting Target URL:  https://www.pcbox.com/categorias/cajas?nodo=16\n",
      "Category:  Torres\n",
      "path exist\n",
      "Requesting Target URL:  https://www.pcbox.com/categorias/tarjetas-graficas?nodo=20\n",
      "Category:  Tarjetas graficas\n",
      "path exist\n",
      "Requesting Target URL:  https://www.pcbox.com/categorias/tarjetas-de-sonido?nodo=28\n",
      "Category:  Tarjetas de sonido\n",
      "path exist\n",
      "Requesting Target URL:  https://www.pcbox.com/categorias/fuentes-de-alimentacion?nodo=281\n",
      "Category:  Fuentes de alimentacion\n",
      "path exist\n",
      "End Timestamp :  2020-11-07 17:24:13\n"
     ]
    }
   ],
   "source": [
    "# Main program\n",
    "#Read config file and initialize vars\n",
    "initialize()\n",
    "#Current Time\n",
    "dateTimeObj = datetime.now()\n",
    "timeStamp = dateTimeObj.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print('Start Timestamp : ', timeStamp)\n",
    "\n",
    "# While there is a Category or Target Url in List that needs to be processed\n",
    "numberCategory=0\n",
    "while numberCategory < len(target_urls):\n",
    "    target_url=target_urls[numberCategory]\n",
    "    category=categories[numberCategory]\n",
    "    print(\"Requesting Target URL: \",target_url)\n",
    "    print(\"Category: \",category)\n",
    "    \n",
    "    soup=send_page_request(target_url,headers,timeOut,minInterval++calcIntervalDelay)\n",
    "    if get_pagination(soup):\n",
    "        for subpages in get_pagination(soup):\n",
    "            soup=send_page_request(subpages,headers,timeOut,minInterval+calcIntervalDelay)\n",
    "            productPage=get_products_page(soup)\n",
    "            #productPage.append(get_products_page(soup))\n",
    "            # Need to download images\n",
    "            download_images(productPage)\n",
    "            to_csv(productPage,csvName)\n",
    "    else:\n",
    "        # No need to iterate in sub-pages - No pagination found\n",
    "        productPage=get_products_page(soup)\n",
    "        # Need to download images\n",
    "        download_images(productPage)\n",
    "        to_csv(productPage,csvName)\n",
    "        \n",
    "    numberCategory += 1\n",
    "\n",
    "#Current TimeStamp\n",
    "dateTimeObj = datetime.now()\n",
    "timeStamp = dateTimeObj.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print('End Timestamp : ', timeStamp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
